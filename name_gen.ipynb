{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from utils.forward_pass import *\n",
    "from utils.backward_pass import *\n",
    "\n",
    "from utils.weight import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79de3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/amitness/gender-data/refs/heads/master/genders.csv\")\n",
    "df = pd.read_csv(\"names.csv\")\n",
    "df = pd.DataFrame(df)\n",
    "# df[\"name\"] = df[\"name\"].apply(lambda x: re.sub(r\"\\(.*?\\)\", \"\", x))\n",
    "names = df.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ebe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for nm in names:\n",
    "  if nm[0] in count:\n",
    "    count[nm[0]] += 1\n",
    "  else:\n",
    "    count[nm[0]] = 1\n",
    "\n",
    "plt.plot(count.keys(), count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7126a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = [\".\"] + sorted(list(set(\"\".join(names))))\n",
    "char_to_ix = {\n",
    "    ch: i for i, ch in enumerate(vowels)\n",
    "}\n",
    "ix_to_char = {\n",
    "    i: ch for i, ch in enumerate(vowels)\n",
    "}\n",
    "ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "for nm in names[:2]:\n",
    "  block = [\".\"] * block_size\n",
    "  nm = nm + \".\"\n",
    "  print(nm)\n",
    "  for ch in nm:\n",
    "    print(block, ch)\n",
    "    block = block[1:] + [ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data, block_size):\n",
    "  X = []\n",
    "  Y = []\n",
    "  for d in data:\n",
    "    block = [0] * block_size\n",
    "    d = d + \".\" # to indicate the end of a nmae\n",
    "    for ch in d:\n",
    "      X.append(block)\n",
    "      Y.append(char_to_ix[ch])\n",
    "      block = block[1:] + [char_to_ix[ch]]\n",
    "  # X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "  X, Y = np.array(X), np.array(Y)\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_names = len(names)\n",
    "num_train, num_test = round(0.8 * total_names), round(0.2 * total_names)\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "X_train, Y_train = generate_data(names[:num_train], block_size)\n",
    "X_test, Y_test = generate_data(names[num_train:], block_size)\n",
    "\n",
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vowels); D = 10; seed = None\n",
    "# seed = 2147483647\n",
    "\n",
    "init_method=\"xavier\"\n",
    "# char_embeddings = torch.randn(V, D)\n",
    "char_embeddings = initialize_weight((V, D), None, seed)\n",
    "\n",
    "hidden = 100\n",
    "\n",
    "W1 = initialize_weight((block_size * D, hidden), init_method, seed)\n",
    "b1 = np.zeros(hidden)\n",
    "\n",
    "W2 = initialize_weight((hidden, V), init_method, seed)\n",
    "b2 = np.zeros(V)\n",
    "\n",
    "# gamma = initialize_weight((hidden,))\n",
    "# beta = initialize_weight((hidden,))\n",
    "\n",
    "gamma = np.ones((hidden,))\n",
    "beta = np.zeros((hidden,))\n",
    "\n",
    "bn_param = {\n",
    "  \"mode\": \"train\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f31539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learning_exp = np.linspace(-3, -1.5, 1000)\n",
    "# learning_rates = 10**learning_exp\n",
    "\n",
    "N, block_size = X_train.shape\n",
    "batch_size = 500\n",
    "\n",
    "loss_history = []\n",
    "# lr_history = []\n",
    "\n",
    "lr = 0.002\n",
    "# embed -> affine -> tanh -> affine -> softmax\n",
    "for i in range(4000):\n",
    "  random_indices = np.random.randint(0, N, (batch_size,))\n",
    "  # word embedding\n",
    "  out, embed_cache = word_embedding_forward(X_train[random_indices], char_embeddings)\n",
    "\n",
    "  # affine\n",
    "  out, cache1 = affine_forward(out, W1, b1)\n",
    "\n",
    "  # batch norm\n",
    "  out, bn1_cache = batchnorm_forward(out, gamma, beta, bn_param)\n",
    "\n",
    "  # tanh\n",
    "  out, tan_cache = tanh_forward(out)\n",
    "\n",
    "  # affine\n",
    "  out, cache2 = affine_forward(out, W2, b2)\n",
    "\n",
    "  loss, dscores = softmax_loss(out, Y_train[random_indices])\n",
    "  loss_history.append(loss)\n",
    "  # lr_history.append(learning_exp[i])\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    print(f\"Loss at iteration: {i} -> {loss}\")\n",
    "\n",
    "  # --------------------------------\n",
    "  #         backward pass\n",
    "  # --------------------------------\n",
    "\n",
    "  dx, dw, db = affine_backward(dscores, cache2)\n",
    "\n",
    "  W2 -= lr * dw\n",
    "  b2 -= lr * db\n",
    "\n",
    "  dx = tanh_backward(dx, tan_cache)\n",
    "\n",
    "  dx, dgamma, dbeta = batchnorm_backward(dx, bn1_cache)\n",
    "\n",
    "  gamma -= lr * dgamma\n",
    "  dbeta -= lr * dbeta\n",
    "\n",
    "  dx, dw, db = affine_backward(dx, cache1)\n",
    "\n",
    "  W1 -= lr * dw\n",
    "  b1 -= lr * db\n",
    "\n",
    "  dx = dx.reshape(batch_size, block_size, D)\n",
    "\n",
    "  dw = word_embedding_backward(dx, embed_cache)\n",
    "\n",
    "  char_embeddings -= lr * dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lr_history, loss_history)\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df634786",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Increasing the batch size helped with variation in names, going from batch size of 100 to 500, however the gradients exploded because the learning rate was too high.\n",
    "So, reduced the learning rate to 0.002 from 0.02. Names are much more plausible looking now. When the batch size was rather smaller, had the problem of most of the names starting \n",
    "with letter \"p\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "np.random.seed(None)\n",
    "num_samples = 10\n",
    "for _ in range(num_samples):\n",
    "  output = []\n",
    "  name = [0] * block_size\n",
    "  while True:\n",
    "    name_ = np.array(name).reshape(1, block_size)\n",
    "\n",
    "    out, _ = word_embedding_forward(name_, char_embeddings)\n",
    "\n",
    "    # affine\n",
    "    out, _ = affine_forward(out, W1, b1)\n",
    "\n",
    "    # batch normalization\n",
    "    bn_param[\"mode\"] = \"test\"\n",
    "    out, _ = batchnorm_forward(out, gamma, beta, bn_param)\n",
    "\n",
    "    # tanh\n",
    "    out, _ = tanh_forward(out)\n",
    "\n",
    "    # affine\n",
    "    out, _ = affine_forward(out, W2, b2)\n",
    "\n",
    "    dscores = softmax_loss(out)\n",
    "    index = np.random.choice(a=V, size=1, p=dscores[0]).item()\n",
    "\n",
    "    if index == 0:\n",
    "      break\n",
    "\n",
    "    name = name[1:] + [index]\n",
    "    output.append(index)\n",
    "  print(''.join(ix_to_char[ix] for ix in output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
